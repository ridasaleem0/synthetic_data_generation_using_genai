# Synthetic Data Generation and Evaluation 

## Overview

This Python script demonstrates the process of generating synthetic data using multiple state of the art machine learning synthesizers available in the SDV (Synthetic Data Vault) library. The script experiments with `GaussianCopulaSynthesizer`, `CTGANSynthesizer`, `CopulaGANSynthesizer`, and `TVAESynthesizer` to create synthetic datasets based on the characteristics and patterns observed in the provided `mle_test_data.csv` file. Each synthesizer is evaluated against the original data to assess its effectiveness in replicating key statistical properties and relational dependencies.

## Prerequisites

Before running the script, ensure you have the following installed:
- Python 3.x
- Required Python packages (`pandas`, `sdv`, etc.). Install them using:
  ```
  pip install pandas sdv
  ```

## Steps to Run the Script

1. **Download the Python file:**
   ```
   cd <folder_name>
   ```

2. **Download the Data:**
   Place your `mle_test_data.csv` file in the root directory of the repository.

3. **Run the Script:**
   Execute the Python script `generate_synthetic_data.py`:
   ```
   python generate_synthetic_data.py
   ```

4. **View Results:**
   - The script will generate synthetic data using each synthesizer and evaluate it against the real data.
   - It will display diagnostic checks, quality evaluation reports, and visual comparisons between real and synthetic data for each synthesizer.

## Script Details

- **Synthesizers Used:** The script uses the following synthesizers from SDV:
  - `GaussianCopulaSynthesizer`
  - `CTGANSynthesizer`
  - `CopulaGANSynthesizer`
  - `TVAESynthesizer`
  
- **Evaluation Criteria:** Synthetic data generated by each synthesizer is evaluated based on:
  - Temporal coherence (e.g., policy dates order).
  - Statistical comparison (e.g., distribution of `sum_insured`, `square_foot_area`, 'num_stories').
  - Row-level coherence (e.g., correspondence between `construction_description` and `oed_construction_code`).

- **Visualization:** The script includes visualizations to compare distributions and correlations between real and synthetic data for each synthesizer.

## Output

- The script saves the generated synthetic data for each synthesizer to separate CSV files (`synthetic_data_<synthesizer_name>.csv`) in the root directory.
- Evaluation results and visualizations are displayed during script execution for each synthesizer.

## Additional Notes

- Adjust parameters in the script (e.g., epochs for CTGANSynthesizer) as needed for your specific dataset and requirements.
- Further fine-tuning model parameters, loss functions, and training strategies would allow customization to match more specific data distribution nuances:
  - Adjusting batch sizes, learning rates, and optimizer settings can influence how well the model replicates data distributions.
  - Tailoring model architectures to handle features like skewed distributions, multi-modal data, or rare events improves fidelity.
- Moreover, developing robust evaluation metrics to assess synthetic data against real data benchmarks would ensure distributional fidelity:
  - Statistical similarity metrics (e.g., Kolmogorov-Smirnov test, Jensen-Shannon divergence) quantify how closely synthetic data matches original distributions.
  - Visualizations and exploratory data analysis help intuitively verify the replication of distributional patterns.
- Iteratively refining models based on validation results and domain-specific insights can enhance the accuracy of synthetic data distribution.
- Ensure proper handling of warnings and error messages during script execution.
- This script is not yet pushed to GitHub.

---
